# -*- coding: utf-8 -*-
"""Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qu_EKhnvOPcfL-dWhfwZvub30Jq-bMVB
"""

import pandas as pd

train = pd.read_csv('/content/twitter_training.csv')
train.head()

test = pd.read_csv('/content/twitter_validation.csv')
test.head()

# Define the column names
column_names = ['id', 'company', 'sentiment', 'review']

# Load the CSV file into a dataframe with specified column names
train = pd.read_csv('twitter_training.csv', header=None, names=column_names, quotechar='"')

# Display the first 3 rows of the train dataframe
train.head()

# Define the column names
column_names = ['id', 'company', 'sentiment', 'review']

# Load the CSV file into a dataframe with specified column names
test = pd.read_csv('twitter_validation.csv', header=None, names=column_names, quotechar='"')

# Display the first 3 rows of the test dataframe
test.head(3)

"""Converting the sentiment to categorical to avoid ordinal relationship"""

train['sentiment'] = train['sentiment'].astype('category')
test['sentiment'] = test['sentiment'].astype('category')

import matplotlib.pyplot as plt
sentiment_counts = train['sentiment'].value_counts()

# Display the counts of distinct sentiment values
print(sentiment_counts)

# Plot the sentiment counts using a pie chart
plt.figure(figsize=(4, 4))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Sentiments')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

"""**Dropping Unncessary Columns**
Because it does contribute to learning

"""

train = train.drop(columns=['id', 'company'])
test = test.drop(columns=['id', 'company'])

print(train.head())
print(test.head())

print(train.shape)
print(test.shape)

"""Data Cleaning: removing null values, noise, duplicate rows etc"""

print(train.isnull().sum())
print(test.isnull().sum())

train.dropna(inplace = True)

print(train.isnull().sum())

print(train.duplicated().sum())
print(test.duplicated().sum())

train.drop_duplicates(inplace=True)
test.drop_duplicates(inplace=True)

print(train.duplicated().sum())
print(test.duplicated().sum())

print(train.shape)
print(test.shape)

"""Mapping"""

# Remove leading/trailing spaces from column names if necessary
train.columns = train.columns.str.strip()
test.columns = test.columns.str.strip()

# Map sentiment labels to numeric values
sentiment_mapping = {
    'Negative': -1,
    'Neutral': 0,
    'Positive': 1,
    'Irrelevant': 2
}

train['sentiment'] = train['sentiment'].map(sentiment_mapping)
test['sentiment'] = test['sentiment'].map(sentiment_mapping)

# Check if the mapping was successful
print(train.head())
print(test.head())

# Convert the 'sentiment' column to type category
train['sentiment'] = train['sentiment'].astype('category')
test['sentiment'] = test['sentiment'].astype('category')

sentiment_counts = train['sentiment'].value_counts()

# Display the counts of distinct sentiment values
print(sentiment_counts)

'''import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences'''

'''vocab_size = 10000
embedding_dim = 16
max_length = 100
trunc_type='post'
padding_type='post'
oov_tok = "<OOV>"'''

'''tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)
tokenizer.fit_on_texts(train['review'])
word_index = tokenizer.word_index'''

'''training_sequences = tokenizer.texts_to_sequences(train)
training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)

testing_sequences = tokenizer.texts_to_sequences(test)
testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)'''

'''from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Initialize Logistic Regression model
model = LogisticRegression()

# Train the model on the TF-IDF features and sentiment labels
model.fit(training_padded, train['sentiment'])

# Predict sentiment labels for test data
predictions = model.predict(testing_padded)

# Evaluate model performance
accuracy = accuracy_score(test['sentiment'], predictions)
print(f'Accuracy: {accuracy:.4f}')

# Display classification report
print('\nClassification Report:')
print(classification_report(test['sentiment'], predictions))'''

'''import numpy as np
training_padded = np.array(training_padded)
testing_padded = np.array(testing_padded)'''

'''from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Initialize Logistic Regression model
model = LogisticRegression()

# Train the model on the TF-IDF features and sentiment labels
# The error was caused by using 'training_sequences' which has shape (69769, 2)
# Instead, use 'training_padded' which is a NumPy array with shape (69769, max_length)
# and contains the padded sequences suitable for training the model.
model.fit(training_padded, train['sentiment'])

# Predict sentiment labels for test data
# Similarly, use 'testing_padded' for predictions instead of 'test_tfidf'
predictions = model.predict(testing_padded)

# Evaluate model performance
accuracy = accuracy_score(test['sentiment'], predictions)
print(f'Accuracy: {accuracy:.4f}')

# Display classification report
print('\nClassification Report:')
print(classification_report(test['sentiment'], predictions))'''

'''#Training our neural network
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dense(4, activation='sigmoid')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])'''

#model.summary()

'''num_epochs = 30
history = model.fit(training_padded, epochs=num_epochs, validation_data=(testing_padded), verbose=2)'''

import nltk

nltk.download('punkt_tab')

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string

# Download NLTK resources (you only need to do this once)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab') # Download the punkt_tab data

# Initialize NLTK's WordNet lemmatizer
lemmatizer = WordNetLemmatizer()

# Define a function for NLP preprocessing
def preprocess_text(text):
    if isinstance(text, str):  # Check if text is a string
        # Tokenization
        tokens = word_tokenize(text)

        # Lowercasing
        tokens = [token.lower() for token in tokens]

        # Removing punctuation
        tokens = [token for token in tokens if token not in string.punctuation]

        # Removing stopwords
        stop_words = set(stopwords.words('english'))
        tokens = [token for token in tokens if token not in stop_words]

        # Lemmatization
        tokens = [lemmatizer.lemmatize(token) for token in tokens]

        # Join the tokens back into a single string
        processed_text = ' '.join(tokens)

        return processed_text
    else:
        return ''  # Return empty string for non-string values

# Apply the preprocessing function to the 'genres_and_tags_str' column
train['review'] = train['review'].apply(preprocess_text)
test['review'] = test['review'].apply(preprocess_text)

# Display the first few rows to verify
print(train.head(3))
print(test.head(3))

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform on the training data
train_tfidf = tfidf_vectorizer.fit_transform(train['review'])

# Transform the test data using the fitted vectorizer
test_tfidf = tfidf_vectorizer.transform(test['review'])

# Display the shape of the transformed data
print("Shape of TF-IDF matrix for training data:", train_tfidf.shape)
print("Shape of TF-IDF matrix for test data:", test_tfidf.shape)

"""**Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# Initialize Logistic Regression model
model = LogisticRegression()

# Train the model on the TF-IDF features and sentiment labels
model.fit(train_tfidf, train['sentiment'])

# Predict sentiment labels for test data
predictions = model.predict(test_tfidf)

# Evaluate model performance
accuracy = accuracy_score(test['sentiment'], predictions)
print(f'Accuracy: {accuracy:.4f}')

# Display classification report
print('\nClassification Report:')
print(classification_report(test['sentiment'], predictions))

from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest model
rf_model = RandomForestClassifier()

# Train the Random Forest model on the TF-IDF features and sentiment labels
rf_model.fit(train_tfidf, train['sentiment'])

# Predict sentiment labels for test data using the trained model
rf_predictions = rf_model.predict(test_tfidf)

# Evaluate Random Forest model performance
rf_accuracy = accuracy_score(test['sentiment'], rf_predictions)
print(f'Random Forest Accuracy: {rf_accuracy:.4f}')

# Display Random Forest classification report
print('\nRandom Forest Classification Report:')
print(classification_report(test['sentiment'], rf_predictions))

"""Saving Model

"""

import pickle

# Save RandomForest model
with open('rf_model.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

# Save train and test dataframes
# train.to_pickle('train_data.pkl')
# test.to_pickle('test_data.pkl')

with open('vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf_vectorizer, f)

print("Models and dataframes saved successfully.")

